{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import (exp, log, mean, std, pi, dot, sqrt, random, arange, array, \n",
    "                   unique, zeros, argmax, append, pad, eye, max, squeeze, \n",
    "                   multiply, sum, divide)\n",
    "### Q1: your code starts here. \n",
    "\n",
    "# Define model structure\n",
    "# each image is 2D, 62x47, with each element defining pixel intensity (0, 255)\n",
    "# since image isn't nxn, we can apply padding to the cols - if needed\n",
    "# image is grayscale input -> depth = 1, input = 64*47 per image\n",
    "\n",
    "# output is # classes or unique y_values\n",
    "\n",
    "# apply(convolve) filter (n x n block) to image -> feature map\n",
    "# loop til min value of cost function -> forward propagation = current loss\n",
    "\n",
    "### padding if needed\n",
    "def padding(x):\n",
    "    x_pad = []\n",
    "    if x.shape[1] != x.shape[2]:\n",
    "    diff = x.shape[1] - x.shape[2]\n",
    "    for i in range(0, x.shape[0]):\n",
    "        x_pad.append(pad(x[i], [(0,0), (0,diff)], mode='constant'))\n",
    "    return array(x_pad)\n",
    "  return x\n",
    "\n",
    "### activation/cost functions\n",
    "#prob norm-dist\n",
    "def p(x, y):\n",
    "    sig = std(y)\n",
    "    mu = mean(y)\n",
    "    return (1.0/(sig*(2*pi)**(0.5)))*exp(-0.5*((x-mu)/sig)**2)\n",
    " \n",
    "def logloss(y):\n",
    "    N = len(y)\n",
    "    init = 0\n",
    "    for i in range(1, N):\n",
    "    init = init + y[i]*log(p(y[i],y)) + 1-y[i]*log(1-p(y[i], y))\n",
    "    return (1.0/-N)*init\n",
    "\n",
    "def sigmoid(z, derivative=False):\n",
    "    sig = 1./(1.+exp(-z))\n",
    "    if derivative:\n",
    "        return sig*(1-sig)\n",
    "    return sig, z\n",
    "\n",
    "def Square_loss(A,Y,derivative=False):\n",
    "\"\"\"Compute square loss or its derivative\"\"\"\n",
    "  error = A-Y\n",
    "    if derivative:\n",
    "        return error\n",
    "\n",
    "    cost = 1/2*dot(error, error) \n",
    "    return cost\n",
    "\n",
    "def Logistic_loss(A,Y, derivative=False):\n",
    "\"\"\"Compute logistic loss or its derivative\"\"\"\n",
    "  if derivative:\n",
    "    num = A - Y\n",
    "    dem = A * (1-A) # this is equal to g'(Z)\n",
    "    return num/dem\n",
    "\n",
    "  cost = -dot(log(A), Y) - dot(log(1-A), 1-Y)\n",
    "    return cost\n",
    "\n",
    "def onehot(y):\n",
    "\"\"\"Returns Hot Encoding\"\"\"\n",
    "    y_vals = max(y)+1\n",
    "    onehot = eye(y_vals)[y]\n",
    "    return squeeze(onehot).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize model's parameters\n",
    "def init_params(layers_dims):\n",
    "    random.seed(3)               \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            \n",
    "\n",
    "    for l in range(1, L):           \n",
    "        parameters[\"W\" + str(l)] = random.randn(\n",
    "          layers_dims[l], layers_dims[l - 1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = zeros((layers_dims[l], 1))\n",
    "        assert parameters[\"W\" + str(l)].shape == (layers_dims[l], layers_dims[l-1])\n",
    "        assert parameters[\"b\" + str(l)].shape == (layers_dims[l], 1)\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\n",
    "            \"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\n",
    "            \"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d306998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    Z = dot(W, A_prev) + b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation_fn=sigmoid):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1])\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, parameters, activation=sigmoid):\n",
    "    A = []\n",
    "    A = X                           \n",
    "    caches = []                     \n",
    "    L = len(parameters) // 2        \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(\n",
    "            A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)],\n",
    "            activation_fn=activation)\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(\n",
    "        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],\n",
    "        activation_fn=sigmoid)\n",
    "    caches.append(cache)\n",
    "\n",
    "    # print(A.shape)\n",
    "    # print(AL.shape)\n",
    "    # print(X.shape[1])\n",
    "    # print((1, X.shape[1]))\n",
    "    assert AL.shape == (5, X.shape[1])\n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "def forward_prop(x, params,activation=sigmoid):\n",
    "    A = x # input to first layer i.e. training data\n",
    "    caches = []\n",
    "    L = len(params)//2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        \n",
    "    # Linear transformation \n",
    "    Z = dot(params['W'+str(l)], A_prev) + params['b'+str(l)] \n",
    "        \n",
    "    # storing the both linear and activation cache\n",
    "    cache = (A_prev, Z)\n",
    "    caches.append(cache)\n",
    "\n",
    "    # Applying activation function on linear transformation\n",
    "    A  = activation(Z) \n",
    "    print(A)\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propagation\n",
    "\n",
    "def sigmoid_gradient(dA, Z):\n",
    "    A, Z = sigmoid(Z)\n",
    "    dZ = dA * A * (1 - A)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def linear_backword(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = dot(W.T, dZ)\n",
    "\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation_fn):\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = sigmoid_gradient(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backword(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, y, caches, activation=sigmoid):\n",
    "    y = y.reshape(AL.shape)\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "\n",
    "    dAL = divide(AL - y, multiply(AL, 1 - AL))\n",
    "\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\n",
    "        \"db\" + str(L)] = linear_activation_backward(\n",
    "            dAL, caches[L - 1], sigmoid)\n",
    "\n",
    "    for l in range(L - 1, 0, -1):\n",
    "        current_cache = caches[l - 1]\n",
    "        grads[\"dA\" + str(l - 1)], grads[\"dW\" + str(l)], grads[\n",
    "            \"db\" + str(l)] = linear_activation_backward(\n",
    "                grads[\"dA\" + str(l)], current_cache, activation)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def backward_prop(AL, Y, caches, params, loss_function=Square_loss, activation=sigmoid):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    AL1, ZL = caches[L-1]\n",
    "    delta_L = loss_function(AL, Y, derivative=True) * activation(ZL, derivative=True)\n",
    "    grads['db'+str(L)] = delta_L\n",
    "    grads['dW'+str(L)] = array([delta_L]).T @ array([AL1])\n",
    "\n",
    "    for l in range(L-1, 0, -1):\n",
    "        delta_curr  = delta_prev\n",
    "        A_prev, Z_curr = caches[l]\n",
    "        W_curr  = params['W'+str(l+2)]\n",
    "        delta_prev = dot(W_curr.T, delta_curr) * activation(Z_curr, derivative=True)\n",
    "        grads['db'+str(l+1)] = delta_prev\n",
    "        grads['dW'+str(l+1)] = array([delta_prev]).T  @ array([A_prev])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51439f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-entropy cost\n",
    "def compute_cost(AL, y):\n",
    "    m = y.shape[1]              \n",
    "    cost = - (1 / m) * sum(\n",
    "        multiply(y, log(AL)) + multiply(1 - y, log(1 - AL)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e262db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "def train_NN(Xtrain,Ytrain_vec, epochs, layer_dim, \n",
    "             learning_rate, cost_function, activation_function):\n",
    "  # Step 2 initialize gradient based algorithm\n",
    "    params = init_params(layer_dim)\n",
    "    N      = Xtrain.shape[1]\n",
    "    cost_span = zeros(epochs)\n",
    "    cost_list = []\n",
    "\n",
    "  # Step 3: training the neural network\n",
    "    arr = arange(N)\n",
    "    for i in range(epochs):\n",
    "        l_rate   = learning_rate[i]\n",
    "        random.shuffle(arr)\n",
    "        cost_i = 0\n",
    "    for j in arr:\n",
    "        Y_hat, caches = L_model_forward(Xtrain, params, \n",
    "                                   activation = activation_function) # sub step 1\n",
    "\n",
    "        cost = compute_cost(Y_hat, Ytrain_vec)\n",
    "\n",
    "        grads = L_model_backward(Y_hat, Ytrain_vec, caches, activation = activation_function) #sub step 2\n",
    "\n",
    "        params = update_parameters(params, grads, l_rate) # sub step 3\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"The cost after {i + 1} iterations is: {cost:.4f}\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost)\n",
    "\n",
    "    cost_span[i] = cost_i/N  \n",
    "    #print(['At epochs ', i, 'the cost is ', cost_i/N])\n",
    "    return params, cost_span\n",
    "\n",
    "def prediction(params, Xtest):\n",
    "    N = Xtest.shape[0]\n",
    "    Ypred = zeros(N)\n",
    "    for j in range(N):\n",
    "        Y_hat, caches = forward_prop(Xtest[j], params, activation = activation_function)\n",
    "        Ypred[j] = argmax(Y_hat)\n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = unique(y_train)\n",
    "N_train = X_train.shape[0]\n",
    "X_train1 = X_train.reshape(N_train, -1).T # num_feat x num_images\n",
    "y_train1 = y_train.reshape(-1, N_train) \n",
    "\n",
    "INPUT_LAYER = X_train1.shape[0]\n",
    "OUTPUT_LAYER = len(unique(y_train1))\n",
    "DEPTH = 5\n",
    "NEURONS = 5\n",
    "layer_dim = [INPUT_LAYER, DEPTH, NEURONS, OUTPUT_LAYER]\n",
    "Y_train_vec = onehot(y_train1) # num_images x num_classes \n",
    "\n",
    "cost_function       = Logistic_loss #Logistic_loss\n",
    "activation_function = sigmoid\n",
    "\n",
    "epochs = 100\n",
    "epochs_span = arange(epochs)\n",
    "learning_rate = 1/((epochs_span+1)**0.1)\n",
    "\n",
    "params = init_params(layer_dim)\n",
    "# X_train.shape\n",
    "print(params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eda6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot(params['W1'], X_train1).shape\n",
    "X_train1.shape\n",
    "Y_train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_Log, cost_span_Log = train_NN(X_train1,Y_train_vec, epochs, layer_dim, \n",
    "                                     learning_rate,cost_function, \n",
    "                                     activation_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e93ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_Log,\n",
    "cost_span_Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, parameters, y, activation_fn=\"relu\"):\n",
    "    probs, caches = L_model_forward(X, parameters, activation_fn)\n",
    "    labels = (probs >= 0.5) * 1\n",
    "    accuracy = mean(labels == y) * 100\n",
    "    return f\"The accuracy rate is: {accuracy:.2f}%.\"\n",
    "\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "X_test1 = X_test.reshape(N_test, -1).T # num_feat x num_images\n",
    "y_test1 = y_test.reshape(-1, N_test) \n",
    "\n",
    "accuracy(X_test1, params_Log, y_test1, activation_fn=\"sigmoid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
